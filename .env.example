DEEPSEEK_API_KEY=your_deepseek_api_key_here
TARGET_LANGUAGE=pt-BR

# --- LOCAL LLM CONFIG (Ollama / LM Studio) ---
# Default for Ollama: http://localhost:11434/v1
# Default for LM Studio: http://localhost:1234/v1
OLLAMA_BASE_URL=http://localhost:11434/v1
# Model name to use locally (e.g., deepseek-r1, llama3.2, mistral)
OLLAMA_MODEL=deepseek-r1

# --- TUNING ---
# Target latency per LLM request in seconds (used to estimate chunk size)
TARGET_LATENCY=4.0
# Heuristic tokens/sec for the model (adjust if you measure a different rate)
TOKENS_PER_SEC=60
# Approximate characters per token (English ~3-4). Used to estimate token counts.
AVG_CHARS_PER_TOKEN=4

# Fixed batch size (number of subtitle blocks per request).
# Set to a positive integer (e.g. 10, 20) to override dynamic chunking.
#BATCH_SIZE=20